# ==================
# Base Image
# ==================
FROM nvidia/cuda:12.4.1-runtime-ubuntu22.04 AS base

WORKDIR /app

# Install system dependencies, Node.js, and Nginx
RUN apt-get update && apt-get install -y --no-install-recommends \
    libstdc++6 libgomp1 openssh-server sudo curl unzip wget vim nvtop \
    python3 python3-pip python3-venv python3-dev git ffmpeg libsm6 libxext6 libgl1 ufw \
    nginx && \
    curl -fsSL https://deb.nodesource.com/setup_18.x | bash - && \
    apt-get install -y nodejs && \
    rm -rf /var/lib/apt/lists/*

# Upgrade pip and install Python build tools
RUN pip3 install --upgrade pip setuptools wheel
# Install Stable Diffusion dependencies
RUN pip3 install torch torchvision torchaudio numpy pillow

# ------------------
# Install backend dependencies
# ------------------
COPY shared/backend/requirements.txt /app/backend/requirements.txt
WORKDIR /app/backend
RUN pip3 install -r requirements.txt

# ------------------
# Install frontend dependencies
# ------------------
WORKDIR /app/frontend
COPY shared/frontend/package.json shared/frontend/package-lock.json ./
RUN npm install

# ------------------
# Prepare server binaries and directories
# ------------------
WORKDIR /app
RUN mkdir -p /app/server

ARG VERSION=v1.1.10
COPY shared/binaries/* /app/

RUN unzip undreamai-${VERSION}-llamacpp.zip -d /app/server/ && \
    unzip undreamai-${VERSION}-server.zip -d /app/server/ && \
    rm undreamai-${VERSION}-llamacpp.zip undreamai-${VERSION}-server.zip

# Create models directories
RUN mkdir -p /models/stable-diffusion /models/llm

# ------------------
# Clone Stable Diffusion WebUI and configure it
# ------------------
RUN git clone https://github.com/AUTOMATIC1111/stable-diffusion-webui.git /app/stable-diffusion-webui

# Create required model directories
RUN mkdir -p /app/stable-diffusion-webui/models/VAE \
    /app/stable-diffusion-webui/models/Lora \
    /app/stable-diffusion-webui/models/Stable-diffusion \
    /app/stable-diffusion-webui/models/ControlNet \
    /app/stable-diffusion-webui/models/ESRGAN \
    /app/stable-diffusion-webui/models/hypernetworks

WORKDIR /app/stable-diffusion-webui
RUN cat > config.yaml <<'EOL'
commandline_args:
  - --listen
  - --port
  - "7860"
  - --allow-code
  - --no-download-sd-model
  - --api
  - --xformers
  - --cors-allow-origins
  - "*"
EOL

# Create a custom launcher script
RUN echo '#!/bin/bash' > /app/stable-diffusion-webui/sd_launcher.sh && \
    echo 'cd /app/stable-diffusion-webui' >> /app/stable-diffusion-webui/sd_launcher.sh && \
    echo 'export PYTHONPATH=/app/stable-diffusion-webui' >> /app/stable-diffusion-webui/sd_launcher.sh && \
    echo '# Create symbolic links to models in the mounted volume' >> /app/stable-diffusion-webui/sd_launcher.sh && \
    echo 'if [ -d "/models/stable-diffusion" ]; then' >> /app/stable-diffusion-webui/sd_launcher.sh && \
    echo '  for file in /models/stable-diffusion/*.safetensors /models/stable-diffusion/*.ckpt; do' >> /app/stable-diffusion-webui/sd_launcher.sh && \
    echo '    if [ -f "$file" ]; then' >> /app/stable-diffusion-webui/sd_launcher.sh && \
    echo '      ln -sf "$file" /app/stable-diffusion-webui/models/Stable-diffusion/' >> /app/stable-diffusion-webui/sd_launcher.sh && \
    echo '      echo "Linked model: $file"' >> /app/stable-diffusion-webui/sd_launcher.sh && \
    echo '    fi' >> /app/stable-diffusion-webui/sd_launcher.sh && \
    echo '  done' >> /app/stable-diffusion-webui/sd_launcher.sh && \
    echo 'fi' >> /app/stable-diffusion-webui/sd_launcher.sh && \
    echo 'python3 launch.py --skip-torch-cuda-test --api --xformers "$@"' >> /app/stable-diffusion-webui/sd_launcher.sh && \
    chmod +x /app/stable-diffusion-webui/sd_launcher.sh && \
    chmod +x /app/stable-diffusion-webui/webui.sh

# Create a helper script for downloading models
RUN echo '#!/bin/bash' > /app/download_models.sh && \
    echo '# Download models script for VelesAI Server' >> /app/download_models.sh && \
    echo 'mkdir -p /models/stable-diffusion' >> /app/download_models.sh && \
    echo 'mkdir -p /models/llm' >> /app/download_models.sh && \
    echo '' >> /app/download_models.sh && \
    echo '# Download a small Stable Diffusion model for testing' >> /app/download_models.sh && \
    echo 'if [ ! -f "/models/stable-diffusion/dreamshaper-8.safetensors" ]; then' >> /app/download_models.sh && \
    echo '  echo "Downloading DreamShaper v8 (lightweight SD model)..."' >> /app/download_models.sh && \
    echo '  wget --show-progress https://huggingface.co/Lykon/dreamshaper-8/resolve/main/dreamshaper_8.safetensors -O /models/stable-diffusion/dreamshaper-8.safetensors' >> /app/download_models.sh && \
    echo 'fi' >> /app/download_models.sh && \
    echo '' >> /app/download_models.sh && \
    echo '# Download a small LLM for testing' >> /app/download_models.sh && \
    echo 'if [ ! -f "/models/llm/phi-2.gguf" ]; then' >> /app/download_models.sh && \
    echo '  echo "Downloading Phi-2 (small LLM model)..."' >> /app/download_models.sh && \
    echo '  wget --show-progress https://huggingface.co/TheBloke/phi-2-GGUF/resolve/main/phi-2.Q5_K_M.gguf -O /models/llm/phi-2.gguf' >> /app/download_models.sh && \
    echo 'fi' >> /app/download_models.sh && \
    echo '' >> /app/download_models.sh && \
    echo 'echo "Model downloads complete!"' >> /app/download_models.sh && \
    chmod +x /app/download_models.sh

# ==================
# Application Stage
# ==================
FROM base AS final

WORKDIR /app

# Copy the latest application source code
COPY shared/backend /app/backend
COPY shared/frontend /app/frontend

# Copy Stable Diffusion WebUI from the base stage
COPY --from=base /app/stable-diffusion-webui /app/stable-diffusion-webui

# Copy Nginx configuration
COPY shared/nginx/nginx.conf /etc/nginx/nginx.conf
COPY shared/nginx/allowed_ips.conf /etc/nginx/allowed_ips.conf

# Ensure models directory exists
RUN mkdir -p /models

# ------------------
# Create RunPod entrypoint script
# ------------------
WORKDIR /app
RUN printf '#!/bin/bash\n\n\
# Ensure directories exist\n\
mkdir -p /run/sshd\n\
mkdir -p /models/stable-diffusion\n\
mkdir -p /models/llm\n\n\
# Configure Nginx IP whitelisting\n\
if [ -z "$ALLOWLIST" ] || [ "$ALLOWWLIST" = "all" ]; then\n\
  echo "# Allow all IPs by default" > /etc/nginx/allowed_ips.conf\n\
  echo "0.0.0.0/0 1;" >> /etc/nginx/allowed_ips.conf\n\
else\n\
  echo "# Generated whitelist from ALLOWED_IPS env var" > /etc/nginx/allowed_ips.conf\n\
  IFS="," read -ra IPS <<< "$ALLOWLIST"\n\
  for ip in "${IPS[@]}"; do\n\
    echo "$ip 1;" >> /etc/nginx/allowed_ips.conf\n\
  done\n\
fi\n\
echo "IP whitelist configuration:"\n\
cat /etc/nginx/allowed_ips.conf\n\n\
# Check for Stable Diffusion models\n\
echo "Checking for Stable Diffusion models in /models/stable-diffusion..."\n\
if [ -d "/models/stable-diffusion" ]; then\n\
  file_count=$(find /models/stable-diffusion -type f \\( -name "*.safetensors" -o -name "*.ckpt" \\) | wc -l)\n\
  if [ "$file_count" -gt 0 ]; then\n\
    echo "Found $file_count SD model(s)"\n\
  else\n\
    echo "No SD models found."\n\
  fi\n\
fi\n\n\
# Check for LLM models\n\
echo "Checking for LLM models in /models/llm..."\n\
if [ -d "/models/llm" ]; then\n\
  file_count=$(find /models/llm -type f -name "*.gguf" | wc -l)\n\
  if [ "$file_count" -gt 0 ]; then\n\
    echo "Found $file_count LLM model(s)"\n\
  else\n\
    echo "No LLM models found."\n\
  fi\n\
fi\n\n\
# Start services\n\
trap "exit" INT TERM\n\
trap "kill 0" EXIT\n\n\
# Backend API\n\
cd /app/backend\n\
echo "Starting backend API on port 8000..."\n\
uvicorn main:app --host 0.0.0.0 --port 8000 &\n\
\n\
# Frontend\n\
cd /app/frontend\n\
echo "Starting frontend on port 3000..."\n\
npm run preview -- --host 0.0.0.0 --port 3000 &\n\n\
# SSH and Nginx\n\
echo "Starting SSH and Nginx..."\n\
/usr/sbin/sshd -D &\n\
nginx\n\n\
# Block on SSH process\n\
wait $!\n' > /app/entrypoint.sh && chmod +x /app/entrypoint.sh

# ------------------
# Build frontend production assets
# ------------------
WORKDIR /app/frontend
RUN npm run build

ENTRYPOINT ["/app/entrypoint.sh"]
